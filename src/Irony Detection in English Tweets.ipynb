{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Irony Detection in English \n",
    "\n",
    "#### The goal of this project consists in training a model capable of performing the task of classification if a tweet in English is ironic or don’t.\n",
    "\n",
    "Using different classification techniques we intend to find the one that performs better using as training data a data set with around 4000 tweets of different types.\n",
    "\n",
    "This project was made by 3rd students as part of the Artificial Intelligence at FEUP. You are free to use the code for any purpose, but beware that this is just an academia project in an introductory course to Artificial Intelligence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets:\n",
    "\n",
    "The data sets for this project are not own by our group, they are supplied by SemEval competition and are free to use for academic purpose only outside of the competition.\n",
    "\n",
    "There are two data sets available, one marked for train other for testing. Originally they were in .txt format, but in order to simplify the work of the pandas library, we manually converted those files to .csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem with word delimeter\n",
    "\n",
    "Pandas is a library perfect for the job of reading the data from a file, since it setup automaticaly the capability of seeing that data in a formatted organized way.\n",
    "\n",
    "Pandas has specific functions to read from different formats. 4000 tweets in JSON would turn the data file really dense in information.\n",
    "Since we are dealing with just 2 attributes is pointless to make our life harder in this step. We choosed .csv format to represent the data\n",
    "\n",
    "CSV files dealing with text rise other problem, the fact that people use comma in their texts, pandas. So we introduce other delimiter that exists in Portuguese, but doesn't exist in english \"_ç\", so we are able to do the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "\n",
    "train_data=pandas.read_csv('data/train.csv', engine='python', encoding=\"utf-8\",delimiter=\"_ç\")\n",
    "test_data=pandas.read_csv('data/test-labeled.csv',engine='python', encoding=\"utf-8\",delimiter=\"_ç\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Sweet United Nations video. Just in time for C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>@mrdahl87 We are rumored to have talked to Erv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Hey there! Nice to see you Minnesota/ND Winter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3 episodes left I'm dying over here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>\"I can't breathe!\" was chosen as the most nota...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                              tweet\n",
       "0      1  Sweet United Nations video. Just in time for C...\n",
       "1      1  @mrdahl87 We are rumored to have talked to Erv...\n",
       "2      1  Hey there! Nice to see you Minnesota/ND Winter...\n",
       "3      0                3 episodes left I'm dying over here\n",
       "4      1  \"I can't breathe!\" was chosen as the most nota..."
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@Callisto1947 Can U Help?||More conservatives ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Just walked in to #Starbucks and asked for a \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>#NOT GONNA WIN http://t.co/Mc9ebqjAqj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>@mickymantell He is exactly that sort of perso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>So much #sarcasm at work mate 10/10 #boring 10...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                              tweet\n",
       "0      0  @Callisto1947 Can U Help?||More conservatives ...\n",
       "1      1  Just walked in to #Starbucks and asked for a \"...\n",
       "2      0              #NOT GONNA WIN http://t.co/Mc9ebqjAqj\n",
       "3      0  @mickymantell He is exactly that sort of perso...\n",
       "4      1  So much #sarcasm at work mate 10/10 #boring 10..."
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words\n",
    "\n",
    "Now that we have the data in a pandas data set we need to proceed our path of finding an accurate model to identify ironic tweets. For that we need to parse our tweets to a **bag of words** model format.\n",
    "\n",
    "A bag of words is a model which tweet will see their words tokenized and counted by a specified formula. Where we have essential 2 paths that we can follow:\n",
    "\n",
    "* Use a simple counting of words.\n",
    "* Use the TF-IDF measure\n",
    "\n",
    "##### The process of getting as input the raw text and output a bag of words is called **Vectorization**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Word Counter Vetorization\n",
    "#### Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "count_vectorizer = CountVectorizer(analyzer='word', stop_words='english', lowercase=False)\n",
    "\n",
    "X_train=count_vectorizer.fit_transform(train_data['tweet']).toarray()\n",
    "y_train=train_data['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we just Transform. Because the vocabulary is the on from the test set\n",
    "X_test=count_vectorizer.transform(test_data['tweet']).toarray()\n",
    "y_test=test_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.42      0.51       473\n",
      "           1       0.42      0.65      0.51       311\n",
      "\n",
      "    accuracy                           0.51       784\n",
      "   macro avg       0.53      0.53      0.51       784\n",
      "weighted avg       0.56      0.51      0.51       784\n",
      "\n",
      "[[199 274]\n",
      " [110 201]]\n"
     ]
    }
   ],
   "source": [
    "classificator=GaussianNB()\n",
    "\n",
    "classificator.fit(X_train,y_train)\n",
    "\n",
    "y_predicted=classificator.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test,y_predicted))\n",
    "print(confusion_matrix(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Word Counter Vetorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_idf_vectorizer=TfidfVectorizer(analyzer='word', stop_words='english', lowercase=False)\n",
    "\n",
    "X_train=tf_idf_vectorizer.fit_transform(train_data['tweet']).toarray()\n",
    "y_train=train_data['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=tf_idf_vectorizer.transform(test_data['tweet']).toarray()\n",
    "y_test=test_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 1 0\n",
      " 1 1 1 1 0 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 1 1 1 1 0 1\n",
      " 0 1 0 0 1 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 1 1 0 1 1 1 1\n",
      " 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 0\n",
      " 0 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 1 1 0 1 1 0 1\n",
      " 1 1 1 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0\n",
      " 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 1 1\n",
      " 1 1 0 1 1 0 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 1 0 0 1\n",
      " 1 0 0 0 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 0 1 0 1 1 1 0 1\n",
      " 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 0 0\n",
      " 0 0 1 0 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1 1 1 0 1 1\n",
      " 0 1 0 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 0 0 0 0\n",
      " 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1 0\n",
      " 1 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 1 1\n",
      " 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 1\n",
      " 0 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1\n",
      " 1 0 0 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 0 1\n",
      " 1 1 1 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 1\n",
      " 0 0 0 1 1 0 1 0 1 1 0 0 0 0 1 0 1 0 1 0 1 1 0 0 1 1 1 0 0 1 1 1 0 1 0 1 0\n",
      " 1 1 1 0 1 1 0 0 1 0 1 0 1 0 1 1 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 0 0 0 0 0\n",
      " 1 1 0 1 0 1 1]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.45      0.53       473\n",
      "           1       0.42      0.60      0.50       311\n",
      "\n",
      "    accuracy                           0.51       784\n",
      "   macro avg       0.53      0.53      0.51       784\n",
      "weighted avg       0.55      0.51      0.52       784\n",
      "\n",
      "[[215 258]\n",
      " [123 188]]\n"
     ]
    }
   ],
   "source": [
    "classificator=GaussianNB()\n",
    "\n",
    "classificator.fit(X_train,y_train)\n",
    "\n",
    "y_predicted=classificator.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test,y_predicted))\n",
    "print(confusion_matrix(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes applied to TF-IDF bag counter bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n",
       "       1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,\n",
       "       1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,\n",
       "       0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "       1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,\n",
       "       0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,\n",
       "       0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,\n",
       "       0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,\n",
       "       0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "       1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,\n",
       "       1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed=tf_idf_vectorizer.transform(test_data['tweet'])\n",
    "\n",
    "new_bag = pandas.DataFrame(data = parsed.toarray(), columns = tf_idf_vectorizer.get_feature_names())\n",
    "\n",
    "model.predict(new_bag)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
